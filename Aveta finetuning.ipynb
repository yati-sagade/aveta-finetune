{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a pre-trained model for a lane boundary following self-driving bot [here](https://rope.donkeycar.com/nets/1/). The model is also stored in ./all_lined_tracks_linear.h5. The idea is to finetune it with a few dense layers and our own dataset in github.com/yati-sagade/aveta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, Input\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_in (InputLayer)             (None, 120, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 58, 78, 24)   1824        img_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 27, 37, 32)   19232       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 12, 17, 64)   51264       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 5, 8, 64)     36928       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 6, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flattened (Flatten)             (None, 1152)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          115300      flattened[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           5050        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "angle_out (Dense)               (None, 15)           765         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "throttle_out (Dense)            (None, 1)            51          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 267,342\n",
      "Trainable params: 267,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/keras/models.py:318: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = load_model('all_lined_tracks_linear.h5')\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to pop out the angle_out and throttle_out tensors and stack two dense layers, of sizes 32, 16, followed by the output layer with two units. The two hidden units can have the sigmoid activation while it is convenient for the output layer units to have tanh activation because of the nice `[-1, 1]` range, which I can map to wheel speeds in either direction quite easily. Note that an output of 1 shall correspond to a speed of 255 and -1 to a speed of -255, which is what the motot HAT library we are using for Aveta expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network():\n",
    "    img_in = Input(shape=(120, 160, 3), name='img_in')\n",
    "\n",
    "    x = Conv2D(24, (5, 5), name='conv2d_1', activation='relu', strides=(2, 2))(img_in)\n",
    "    x = Conv2D(32, (5, 5), name='conv2d_2', activation='relu', strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (5, 5), name='conv2d_3', activation='relu', strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), name='conv2d_4', activation='relu', strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), name='conv2d_5', activation='relu', strides=(1, 1))(x)\n",
    "    x = Flatten(name='flattened')(x)\n",
    "    x = Dense(100, activation='relu', name='dense_1')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout_1')(x)\n",
    "    x = Dense(50, activation='relu', name='dense_2')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout_2')(x)\n",
    "    # This is the beginning of our additions\n",
    "    x = Dense(32, activation='sigmoid', name='dense_3')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout_3')(x)\n",
    "    x = Dense(16, activation='sigmoid', name='dense_4')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout_4')(x)\n",
    "\n",
    "    speeds_out = Dense(2, activation='tanh', name='dense_5')(x)\n",
    "\n",
    "    model = Model(inputs=[img_in], outputs=[speeds_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_in (InputLayer)          (None, 120, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 58, 78, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 37, 32)        19232     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 17, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flattened (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 268,720\n",
      "Trainable params: 268,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not know of a way to generate python code from a stored Keras model, so I had to dig into the JSON representation of the model and write out the model by hand. Of course I've left out the last two layers of the original model, and instead (after the dropout_2 layer) added my own dense layers. I don't have a huge dataset, so I've included dropout layers to guard against overfitting. This may not be needed.\n",
    "\n",
    "Now to transfer the weights of the pretrained model onto our model's first layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_in (InputLayer)          (None, 120, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 58, 78, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 37, 32)        19232     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 17, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flattened (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 268,720\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 266,526\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.name == 'dense_3':\n",
    "        break # Starting here, it's our layers\n",
    "    weights = pretrained_model.get_layer(name=layer.name).get_weights()\n",
    "    layer.set_weights(weights)\n",
    "    layer.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to massage the training data so that we have the following:\n",
    "- A suitable train/test split\n",
    "- Speeds are in speeds.txt, and need to be normalized in the `[-1, 1]` by simply dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/ipykernel_launcher.py:24: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/ipykernel_launcher.py:24: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: train_x: (90, 120, 160, 3), train_y: (90, 2), test_x: (10, 120, 160, 3), test_y: (10, 2)\n",
      "0.011764705882352941 0.49411764705882355\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TEST_MODE = True\n",
    "AVETA_DATA_DIR = os.path.expanduser('~/aveta-data')\n",
    "\n",
    "imdir = os.path.join(AVETA_DATA_DIR, 'generated')\n",
    "imfiles = [f for f in os.listdir(imdir) if f.endswith('.jpg')]\n",
    "random.shuffle(imfiles)\n",
    "\n",
    "if TEST_MODE:\n",
    "    imfiles = imfiles[:100]\n",
    "\n",
    "speedfile = os.path.join(imdir, 'speeds.txt')\n",
    "speeds = {}\n",
    "with open(speedfile) as fp:\n",
    "    for line in fp:\n",
    "        imfile, _, _, left_speed, right_speed = line.strip().split(',')\n",
    "        speeds[imfile] = [left_speed, right_speed]\n",
    "    \n",
    "nbtest = int(len(imfiles)*0.1)\n",
    "test_filenames, train_filenames = imfiles[:nbtest], imfiles[nbtest:]\n",
    "test_x = np.array([\n",
    "    imresize(imread(os.path.join(imdir, f)), (120, 160))\n",
    "    for f in test_filenames\n",
    "])\n",
    "train_x = np.array([\n",
    "    imresize(imread(os.path.join(imdir, f)), (120, 160))\n",
    "    for f in train_filenames\n",
    "])\n",
    "test_y, train_y = [np.array([speeds[f] for f in fileset], dtype=np.float) / 255.0\n",
    "                    for fileset in (test_filenames, train_filenames)]\n",
    "\n",
    "print('Shapes: train_x: {}, train_y: {}, test_x: {}, test_y: {}'.format(\n",
    "    train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "));\n",
    "\n",
    "print(train_y.min(), train_y.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the model can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-54499b6e1d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           batch_size=batch_size)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ys/.virtualenvs/aveta-py3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                                check_array_lengths=True, batch_size=None):\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "checkpoint_filename = 'aveta-checkpoint-{}'.format(now.strftime('%Y%m%d%H%M%S'))\n",
    "save_best = ModelCheckpoint(checkpoint_filename,\n",
    "                            save_best_only=True,\n",
    "                            verbose=1,\n",
    "                            mode='min')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=5,\n",
    "                           verbose=0,\n",
    "                           mode='auto')\n",
    "\n",
    "cbs = [save_best, early_stop]\n",
    "\n",
    "batch_size = 320 if not TEST_MODE else 64\n",
    "model.compile\n",
    "model.fit(train_x,\n",
    "          train_y,\n",
    "          nb_epoch=100,\n",
    "          validation_split=0.1,\n",
    "          callbacks=cbs,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "test_eval = model.evaluate(test_x, test_y, batch_size=batch_size)\n",
    "print(test_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
